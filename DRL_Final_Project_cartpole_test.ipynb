{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DRL_Final_Project_cartpole_test.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOYoMKiatk4r6EtG+HqlWvA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ManikantaMandlem/Deep-Q-Network-to-play-2048-Game/blob/main/DRL_Final_Project_cartpole_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try on carpole problem to see if things are working or not - this should be an easy thing to do\n"
      ],
      "metadata": {
        "id": "dcwVbSm3ljeM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7Yb2M5rtr0g",
        "outputId": "cf5d20a9-6589-461a-d9db-ac93816fafa0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym-2048 in /usr/local/lib/python3.7/dist-packages (0.2.6)\n",
            "Requirement already satisfied: numpy~=1.14.0 in /usr/local/lib/python3.7/dist-packages (from gym-2048) (1.14.6)\n",
            "Requirement already satisfied: gym~=0.10.0 in /usr/local/lib/python3.7/dist-packages (from gym-2048) (0.10.11)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym~=0.10.0->gym-2048) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gym~=0.10.0->gym-2048) (1.15.0)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.7/dist-packages (from gym~=0.10.0->gym-2048) (2.23.0)\n",
            "Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym~=0.10.0->gym-2048) (1.5.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet>=1.2.0->gym~=0.10.0->gym-2048) (0.16.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->gym~=0.10.0->gym-2048) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->gym~=0.10.0->gym-2048) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->gym~=0.10.0->gym-2048) (2022.5.18.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->gym~=0.10.0->gym-2048) (1.24.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install gym-2048"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym_2048\n",
        "import gym\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from collections import deque\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from random import sample\n",
        "import matplotlib.pyplot as plt\n",
        "import time"
      ],
      "metadata": {
        "id": "sQ3ULMj_7smZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#defining a neural network class for network initializations\n",
        "\n",
        "class torch_model(nn.Module):\n",
        "    def __init__(self,layers,dropout=0,normalization=False,hidden_activation='relu'):\n",
        "        super(torch_model,self).__init__()\n",
        "        self.layers = layers\n",
        "        self.dropout = dropout\n",
        "        self.normalization = normalization\n",
        "        self.hidden_activation = hidden_activation\n",
        "\n",
        "        #defining different layers\n",
        "        self.nn_layers = []\n",
        "        for i in range(len(self.layers)-2):\n",
        "            self.nn_layers.append(nn.Linear(self.layers[i],self.layers[i+1]))\n",
        "            if hidden_activation == 'relu':\n",
        "                self.nn_layers.append(nn.ReLU())\n",
        "            elif hidden_activation == 'sigmoid':\n",
        "                self.nn_layers.append(nn.Sigmoid())\n",
        "            elif hidden_activation == 'silu':\n",
        "                self.nn_layers.append(nn.SiLU())\n",
        "            elif hidden_activation == 'tanh':\n",
        "                self.nn_layers.append(nn.Tanh())\n",
        "            elif hidden_activation == 'celu':\n",
        "                self.nn_layers.append(nn.CELU())\n",
        "            else:\n",
        "                raise Exception('activation function not recognized; available options are relu, sigmoid, silu, tanh, celu')\n",
        "            if normalization:\n",
        "                self.nn_layers.append(nn.BatchNorm1d(self.layers[i+1]))\n",
        "            self.nn_layers.append(nn.Dropout(self.dropout))\n",
        "        self.nn_layers.append(nn.Linear(self.layers[-2],self.layers[-1]))\n",
        "        self.nn_layers = nn.ModuleList(self.nn_layers)\n",
        "    def forward(self,x):\n",
        "        for layer in self.nn_layers:\n",
        "            x = layer(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "zS2RFKBN8wKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_model(model):\n",
        "\n",
        "    # Print model's state_dict\n",
        "    print(\"Model's state dictionary (stored weights):\")\n",
        "    for param_tensor in model.state_dict():\n",
        "        print(\"  \", param_tensor, \"\\t\", tuple(model.state_dict()[param_tensor].size()))\n",
        "\n",
        "    # Print the number of parameters in the model    \n",
        "    parameter_count =  sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(\"In total, this network has \", parameter_count, \" trainable parameters\")"
      ],
      "metadata": {
        "id": "X17xktlWeOGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class rl_agent():\n",
        "    def __init__(self,kwargs):\n",
        "        self.environment = gym.make('CartPole-v1')\n",
        "        self.state_size = self.environment.observation_space.shape[0]#*self.environment.observation_space.shape[1]\n",
        "        self.action_size = self.environment.action_space.n\n",
        "        self.episodes = kwargs['episodes']\n",
        "        self.memory = deque(maxlen=kwargs['memory_length'])\n",
        "        self.gamma = kwargs['gamma']    # discount rate\n",
        "        self.epsilon = kwargs['epsilon_start']  # exploration rate\n",
        "        self.epsilon_min = kwargs['epsilon_min']\n",
        "        self.epsilon_decay = kwargs['epsilon_decay']\n",
        "        self.batch_size = kwargs['batch_size']\n",
        "        self.train_start = kwargs['train_start']\n",
        "        self.model = torch_model(layers=kwargs['layers'],dropout=kwargs['dropout'],\n",
        "                                 normalization=kwargs['batch_norm'],hidden_activation=kwargs['activation'])\n",
        "        # print_model(self.model)\n",
        "        #defining memory to remember states, next_states, rewards, actions\n",
        "        self.memory = deque(maxlen = 10000)\n",
        "        self.loss_fn = nn.MSELoss()\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=kwargs['lr'])\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "        if len(self.memory) > self.train_start and self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "    \n",
        "    #training the neural network to approximate the q function\n",
        "    def replay(self):\n",
        "        if len(self.memory) < self.train_start: #if enough examples are not present, then do not train the neural network\n",
        "            # print('model is not training')\n",
        "            return\n",
        "        # Randomly sample minibatch from the memory\n",
        "        minibatch = sample(self.memory, self.batch_size)\n",
        "        state = torch.zeros((self.batch_size, self.state_size))\n",
        "        next_state = torch.zeros((self.batch_size, self.state_size))\n",
        "        action, reward, done = [], [], []\n",
        "        # assign data into state, next_state, action, reward and done from minibatch\n",
        "        for i in range(self.batch_size):\n",
        "            state[i] = minibatch[i][0]\n",
        "            next_state[i] = minibatch[i][3]\n",
        "            action.append(minibatch[i][1])\n",
        "            reward.append(minibatch[i][2])\n",
        "            done.append(minibatch[i][4])\n",
        "        # compute value function of current(call it target) and value function of next state(call it target_next)\n",
        "        self.model.train()\n",
        "        q_target = self.model(state)\n",
        "        q_target_next = self.model(next_state)\n",
        "        for i in range(self.batch_size):\n",
        "            # correction on the Q value for the action used,\n",
        "            # if done[i] is true, then the target should be just the final reward\n",
        "            if done[i]:\n",
        "                q_target[i][action[i]] = reward[i]\n",
        "            else:\n",
        "                # else, use Bellman Equation\n",
        "                # Standard - DQN\n",
        "                # DQN chooses the max Q value among next actions\n",
        "                # selection and evaluation of action is on the target Q Network\n",
        "                # target = max_a' (r + gamma*Q_target_next(s', a'))\n",
        "                q_target[i][action[i]] = reward[i] + self.gamma * torch.max(q_target_next[i])\n",
        "        # Train the Neural Network with batches where target is the value function\n",
        "        # self.model.fit(state, q_target, batch_size=self.batch_size, verbose=0)\n",
        "        # print('model is training')\n",
        "        self.optimizer.zero_grad()\n",
        "        q_values = self.model(state)\n",
        "        loss = self.loss_fn(q_values,q_target)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "\n",
        "    def training(self):\n",
        "        for episode in range(self.episodes):\n",
        "            state = torch.from_numpy(self.environment.reset()) #resetting the game environment and capturing the first state of the game\n",
        "            state = torch.reshape(state,(1,self.state_size))\n",
        "            state = state.float()\n",
        "            # print(state)\n",
        "            # print(type(state))\n",
        "            # print(state.shape)\n",
        "            done = False\n",
        "            acc_reward = 0\n",
        "            i = 0\n",
        "            while not done: #untill the game is not over, play the game\n",
        "                #take the epsilon greedy action\n",
        "                # with torch.no_grad():\n",
        "                # self.environment.render()\n",
        "                # time.sleep(5)\n",
        "                self.model.eval()\n",
        "                q_values = self.model(state)\n",
        "                greedy_action = torch.argmax(q_values).item()\n",
        "                random_action = np.random.choice([0,1])\n",
        "                final_action = np.random.choice([greedy_action,random_action],p=[1-self.epsilon,self.epsilon])\n",
        "                #run the selected action and get the next state, reward, done\n",
        "                next_state, reward, done, _ = self.environment.step(final_action)\n",
        "                next_state = torch.from_numpy(next_state.copy())\n",
        "                next_state = torch.reshape(next_state, (1, self.state_size))\n",
        "                next_state = next_state.float()\n",
        "                #may need to change and test other rewarding methods like max(tiles), difference between max tile and 2048 etc\n",
        "                if not done or i == self.environment._max_episode_steps-1:\n",
        "                    reward = reward # Reward --> +1\n",
        "                else:\n",
        "                    reward = -100 # Reward = -100\n",
        "                #storing the state-action-nextstate-reward-done touple in memory for training purpose\n",
        "                # acc_reward += reward\n",
        "                self.remember(state, final_action, reward, next_state, done)\n",
        "                # print(reward)\n",
        "                #updating the state to next_state\n",
        "                state = next_state\n",
        "                i = i+1\n",
        "                if done:\n",
        "                    #return_rewards.append(i)\n",
        "                    dateTimeObj = datetime.now()\n",
        "                    timestampStr = dateTimeObj.strftime(\"%H:%M:%S\")\n",
        "                    print(\"episode: {}/{}, score: {}, e: {:.2}, time: {}\".format(episode+1, self.episodes, i,self.epsilon, timestampStr))\n",
        "                    # save model option\n",
        "                    # if i >= 500:\n",
        "                    #     print(\"Saving trained model as cartpole-dqn-training.h5\")\n",
        "                    #     self.save(\"./save/cartpole-dqn-training.h5\")\n",
        "                    #     return # remark this line if you want to train the model longer\n",
        "                self.replay()\n",
        "\n",
        "        "
      ],
      "metadata": {
        "id": "q406ZuWYwftx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kwargs = {\n",
        "    'episodes':200,\n",
        "    'gamma':0.95, # changed\n",
        "    'epsilon_start':1.0,\n",
        "    'epsilon_min':0.001, # changed from 0.01\n",
        "    'epsilon_decay':0.999, # changed to 0.995\n",
        "    'batch_size':64,\n",
        "    'train_start':1000,\n",
        "    'layers':[4,512,256,64,2],\n",
        "    'dropout':0.0,\n",
        "    'batch_norm':True, #changed from true\n",
        "    'activation':'relu',\n",
        "    'lr':0.00025, #changed from 0.001\n",
        "    'memory_length':2000\n",
        "}"
      ],
      "metadata": {
        "id": "DhlseyjssQ0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # rewards_final = []\n",
        "    # runs = 20\n",
        "    # for run in range(runs):\n",
        "    agent = rl_agent(kwargs)\n",
        "    agent.training()\n",
        "        # rewards_final.append(agent.training())\n",
        "    # rewards_final  =np.array(rewards_final)\n",
        "    # rewards_avg = np.average(rewards_final,axis = 0)\n",
        "    # rewards_std = np.std(rewards_final,axis = 0)\n",
        "    # plt.plot(list(range(1,151)),rewards_avg,color = 'red',label='average')\n",
        "    # plt.fill_between(list(range(1,151)),rewards_avg-rewards_std,rewards_avg+rewards_std,color='red',label='+/- 1 std',alpha=0.2)\n",
        "    # plt.xlabel('episode #')\n",
        "    # plt.ylabel('reward')\n",
        "    # plt.title('mean and std over 100 runs of reward')\n",
        "    # plt.legend()\n",
        "    # plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AJZjR2hkrzBg",
        "outputId": "7dfb8b66-2fc9-4795-82c8-733ee512be3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "episode: 1/200, score: 39, e: 1.0, time: 00:52:17\n",
            "episode: 2/200, score: 12, e: 1.0, time: 00:52:17\n",
            "episode: 3/200, score: 51, e: 1.0, time: 00:52:17\n",
            "episode: 4/200, score: 28, e: 1.0, time: 00:52:17\n",
            "episode: 5/200, score: 20, e: 1.0, time: 00:52:17\n",
            "episode: 6/200, score: 12, e: 1.0, time: 00:52:17\n",
            "episode: 7/200, score: 30, e: 1.0, time: 00:52:17\n",
            "episode: 8/200, score: 22, e: 1.0, time: 00:52:17\n",
            "episode: 9/200, score: 46, e: 1.0, time: 00:52:17\n",
            "episode: 10/200, score: 28, e: 1.0, time: 00:52:17\n",
            "episode: 11/200, score: 51, e: 1.0, time: 00:52:17\n",
            "episode: 12/200, score: 16, e: 1.0, time: 00:52:17\n",
            "episode: 13/200, score: 12, e: 1.0, time: 00:52:17\n",
            "episode: 14/200, score: 10, e: 1.0, time: 00:52:17\n",
            "episode: 15/200, score: 24, e: 1.0, time: 00:52:17\n",
            "episode: 16/200, score: 26, e: 1.0, time: 00:52:17\n",
            "episode: 17/200, score: 19, e: 1.0, time: 00:52:17\n",
            "episode: 18/200, score: 15, e: 1.0, time: 00:52:17\n",
            "episode: 19/200, score: 17, e: 1.0, time: 00:52:17\n",
            "episode: 20/200, score: 13, e: 1.0, time: 00:52:17\n",
            "episode: 21/200, score: 34, e: 1.0, time: 00:52:17\n",
            "episode: 22/200, score: 20, e: 1.0, time: 00:52:17\n",
            "episode: 23/200, score: 19, e: 1.0, time: 00:52:17\n",
            "episode: 24/200, score: 24, e: 1.0, time: 00:52:17\n",
            "episode: 25/200, score: 13, e: 1.0, time: 00:52:17\n",
            "episode: 26/200, score: 32, e: 1.0, time: 00:52:17\n",
            "episode: 27/200, score: 25, e: 1.0, time: 00:52:17\n",
            "episode: 28/200, score: 22, e: 1.0, time: 00:52:17\n",
            "episode: 29/200, score: 17, e: 1.0, time: 00:52:17\n",
            "episode: 30/200, score: 15, e: 1.0, time: 00:52:17\n",
            "episode: 31/200, score: 19, e: 1.0, time: 00:52:17\n",
            "episode: 32/200, score: 12, e: 1.0, time: 00:52:17\n",
            "episode: 33/200, score: 17, e: 1.0, time: 00:52:17\n",
            "episode: 34/200, score: 14, e: 1.0, time: 00:52:17\n",
            "episode: 35/200, score: 11, e: 1.0, time: 00:52:17\n",
            "episode: 36/200, score: 17, e: 1.0, time: 00:52:17\n",
            "episode: 37/200, score: 16, e: 1.0, time: 00:52:17\n",
            "episode: 38/200, score: 21, e: 1.0, time: 00:52:17\n",
            "episode: 39/200, score: 14, e: 1.0, time: 00:52:18\n",
            "episode: 40/200, score: 17, e: 1.0, time: 00:52:18\n",
            "episode: 41/200, score: 21, e: 1.0, time: 00:52:18\n",
            "episode: 42/200, score: 18, e: 1.0, time: 00:52:18\n",
            "episode: 43/200, score: 17, e: 1.0, time: 00:52:18\n",
            "episode: 44/200, score: 14, e: 1.0, time: 00:52:18\n",
            "episode: 45/200, score: 19, e: 1.0, time: 00:52:18\n",
            "episode: 46/200, score: 11, e: 1.0, time: 00:52:18\n",
            "episode: 47/200, score: 10, e: 1.0, time: 00:52:18\n",
            "episode: 48/200, score: 18, e: 1.0, time: 00:52:18\n",
            "episode: 49/200, score: 14, e: 0.99, time: 00:52:18\n",
            "episode: 50/200, score: 12, e: 0.98, time: 00:52:18\n",
            "episode: 51/200, score: 22, e: 0.96, time: 00:52:18\n",
            "episode: 52/200, score: 15, e: 0.94, time: 00:52:19\n",
            "episode: 53/200, score: 22, e: 0.92, time: 00:52:19\n",
            "episode: 54/200, score: 15, e: 0.91, time: 00:52:19\n",
            "episode: 55/200, score: 21, e: 0.89, time: 00:52:20\n",
            "episode: 56/200, score: 28, e: 0.86, time: 00:52:20\n",
            "episode: 57/200, score: 13, e: 0.85, time: 00:52:20\n",
            "episode: 58/200, score: 15, e: 0.84, time: 00:52:20\n",
            "episode: 59/200, score: 20, e: 0.82, time: 00:52:21\n",
            "episode: 60/200, score: 39, e: 0.79, time: 00:52:21\n",
            "episode: 61/200, score: 13, e: 0.78, time: 00:52:22\n",
            "episode: 62/200, score: 13, e: 0.77, time: 00:52:22\n",
            "episode: 63/200, score: 22, e: 0.75, time: 00:52:22\n",
            "episode: 64/200, score: 21, e: 0.74, time: 00:52:23\n",
            "episode: 65/200, score: 16, e: 0.73, time: 00:52:23\n",
            "episode: 66/200, score: 22, e: 0.71, time: 00:52:23\n",
            "episode: 67/200, score: 13, e: 0.7, time: 00:52:23\n",
            "episode: 68/200, score: 11, e: 0.69, time: 00:52:24\n",
            "episode: 69/200, score: 21, e: 0.68, time: 00:52:24\n",
            "episode: 70/200, score: 11, e: 0.67, time: 00:52:24\n",
            "episode: 71/200, score: 28, e: 0.65, time: 00:52:25\n",
            "episode: 72/200, score: 15, e: 0.64, time: 00:52:25\n",
            "episode: 73/200, score: 21, e: 0.63, time: 00:52:25\n",
            "episode: 74/200, score: 14, e: 0.62, time: 00:52:25\n",
            "episode: 75/200, score: 15, e: 0.61, time: 00:52:26\n",
            "episode: 76/200, score: 15, e: 0.6, time: 00:52:26\n",
            "episode: 77/200, score: 19, e: 0.59, time: 00:52:26\n",
            "episode: 78/200, score: 16, e: 0.58, time: 00:52:26\n",
            "episode: 79/200, score: 9, e: 0.58, time: 00:52:27\n",
            "episode: 80/200, score: 20, e: 0.57, time: 00:52:27\n",
            "episode: 81/200, score: 15, e: 0.56, time: 00:52:27\n",
            "episode: 82/200, score: 16, e: 0.55, time: 00:52:27\n",
            "episode: 83/200, score: 10, e: 0.54, time: 00:52:28\n",
            "episode: 84/200, score: 21, e: 0.53, time: 00:52:28\n",
            "episode: 85/200, score: 11, e: 0.53, time: 00:52:28\n",
            "episode: 86/200, score: 34, e: 0.51, time: 00:52:29\n",
            "episode: 87/200, score: 18, e: 0.5, time: 00:52:29\n",
            "episode: 88/200, score: 20, e: 0.49, time: 00:52:29\n",
            "episode: 89/200, score: 21, e: 0.48, time: 00:52:30\n",
            "episode: 90/200, score: 14, e: 0.47, time: 00:52:30\n",
            "episode: 91/200, score: 33, e: 0.46, time: 00:52:30\n",
            "episode: 92/200, score: 11, e: 0.45, time: 00:52:31\n",
            "episode: 93/200, score: 17, e: 0.44, time: 00:52:31\n",
            "episode: 94/200, score: 16, e: 0.44, time: 00:52:31\n",
            "episode: 95/200, score: 10, e: 0.43, time: 00:52:31\n",
            "episode: 96/200, score: 14, e: 0.43, time: 00:52:32\n",
            "episode: 97/200, score: 49, e: 0.41, time: 00:52:32\n",
            "episode: 98/200, score: 11, e: 0.4, time: 00:52:32\n",
            "episode: 99/200, score: 14, e: 0.4, time: 00:52:33\n",
            "episode: 100/200, score: 49, e: 0.38, time: 00:52:34\n",
            "episode: 101/200, score: 13, e: 0.37, time: 00:52:34\n",
            "episode: 102/200, score: 21, e: 0.37, time: 00:52:34\n",
            "episode: 103/200, score: 22, e: 0.36, time: 00:52:35\n",
            "episode: 104/200, score: 10, e: 0.35, time: 00:52:35\n",
            "episode: 105/200, score: 58, e: 0.33, time: 00:52:36\n",
            "episode: 106/200, score: 19, e: 0.33, time: 00:52:36\n",
            "episode: 107/200, score: 14, e: 0.32, time: 00:52:36\n",
            "episode: 108/200, score: 45, e: 0.31, time: 00:52:37\n",
            "episode: 109/200, score: 22, e: 0.3, time: 00:52:37\n",
            "episode: 110/200, score: 10, e: 0.3, time: 00:52:37\n",
            "episode: 111/200, score: 12, e: 0.3, time: 00:52:38\n",
            "episode: 112/200, score: 50, e: 0.28, time: 00:52:38\n",
            "episode: 113/200, score: 14, e: 0.28, time: 00:52:39\n",
            "episode: 114/200, score: 17, e: 0.27, time: 00:52:39\n",
            "episode: 115/200, score: 37, e: 0.26, time: 00:52:39\n",
            "episode: 116/200, score: 12, e: 0.26, time: 00:52:40\n",
            "episode: 117/200, score: 30, e: 0.25, time: 00:52:40\n",
            "episode: 118/200, score: 22, e: 0.25, time: 00:52:41\n",
            "episode: 119/200, score: 13, e: 0.24, time: 00:52:41\n",
            "episode: 120/200, score: 62, e: 0.23, time: 00:52:42\n",
            "episode: 121/200, score: 18, e: 0.22, time: 00:52:42\n",
            "episode: 122/200, score: 11, e: 0.22, time: 00:52:42\n",
            "episode: 123/200, score: 18, e: 0.22, time: 00:52:42\n",
            "episode: 124/200, score: 31, e: 0.21, time: 00:52:43\n",
            "episode: 125/200, score: 18, e: 0.21, time: 00:52:43\n",
            "episode: 126/200, score: 58, e: 0.2, time: 00:52:44\n",
            "episode: 127/200, score: 15, e: 0.19, time: 00:52:44\n",
            "episode: 128/200, score: 20, e: 0.19, time: 00:52:45\n",
            "episode: 129/200, score: 28, e: 0.18, time: 00:52:45\n",
            "episode: 130/200, score: 54, e: 0.17, time: 00:52:46\n",
            "episode: 131/200, score: 22, e: 0.17, time: 00:52:46\n",
            "episode: 132/200, score: 33, e: 0.16, time: 00:52:47\n",
            "episode: 133/200, score: 45, e: 0.16, time: 00:52:48\n",
            "episode: 134/200, score: 15, e: 0.16, time: 00:52:48\n",
            "episode: 135/200, score: 46, e: 0.15, time: 00:52:49\n",
            "episode: 136/200, score: 56, e: 0.14, time: 00:52:50\n",
            "episode: 137/200, score: 53, e: 0.13, time: 00:52:50\n",
            "episode: 138/200, score: 26, e: 0.13, time: 00:52:51\n",
            "episode: 139/200, score: 47, e: 0.12, time: 00:52:52\n",
            "episode: 140/200, score: 55, e: 0.12, time: 00:52:52\n",
            "episode: 141/200, score: 75, e: 0.11, time: 00:52:54\n",
            "episode: 142/200, score: 37, e: 0.1, time: 00:52:54\n",
            "episode: 143/200, score: 51, e: 0.099, time: 00:52:55\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-38e69a66c3e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# for run in range(runs):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrl_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;31m# rewards_final.append(agent.training())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# rewards_final  =np.array(rewards_final)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-33-192a37d96dca>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    116\u001b[0m                     \u001b[0;31m#     self.save(\"./save/cartpole-dqn-training.h5\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                     \u001b[0;31m#     return # remark this line if you want to train the model longer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-33-192a37d96dca>\u001b[0m in \u001b[0;36mreplay\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0;31m# selection and evaluation of action is on the target Q Network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[0;31m# target = max_a' (r + gamma*Q_target_next(s', a'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m                 \u001b[0mq_target\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_target_next\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0;31m# Train the Neural Network with batches where target is the value function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;31m# self.model.fit(state, q_target, batch_size=self.batch_size, verbose=0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}