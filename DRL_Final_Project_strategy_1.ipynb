{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ManikantaMandlem/Deep-Q-Network-to-play-2048-Game/blob/main/DRL_Final_Project_strategy_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcwVbSm3ljeM"
      },
      "source": [
        "change reward policy - max tile - not working\n",
        "reward policy - max tile at corners - check\n",
        "reward policy - tiles with big numbers to the adjacent - check\n",
        "reward policy - step by step rewarding policy - figure out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym-2048"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EjMyEAPLzq96",
        "outputId": "b06b0046-4033-4464-bd99-ca7947e41607"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gym-2048\n",
            "  Downloading gym-2048-0.2.6.tar.gz (4.6 kB)\n",
            "Collecting gym~=0.10.0\n",
            "  Downloading gym-0.10.11.tar.gz (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 7.7 MB/s \n",
            "\u001b[?25hCollecting numpy~=1.14.0\n",
            "  Downloading numpy-1.14.6-cp37-cp37m-manylinux1_x86_64.whl (13.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.8 MB 22.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym~=0.10.0->gym-2048) (1.4.1)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.7/dist-packages (from gym~=0.10.0->gym-2048) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gym~=0.10.0->gym-2048) (1.15.0)\n",
            "Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym~=0.10.0->gym-2048) (1.5.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet>=1.2.0->gym~=0.10.0->gym-2048) (0.16.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->gym~=0.10.0->gym-2048) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->gym~=0.10.0->gym-2048) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->gym~=0.10.0->gym-2048) (2022.5.18.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->gym~=0.10.0->gym-2048) (3.0.4)\n",
            "Building wheels for collected packages: gym-2048, gym\n",
            "  Building wheel for gym-2048 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym-2048: filename=gym_2048-0.2.6-py3-none-any.whl size=4697 sha256=7561058dfdf18971b38ac42bd862205a2a9eb2600132e728fc7b5f8145b60c3c\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/98/9f/c396b6407bd4c0906c2a6ed5905202cd0d423dd2d6d8db05a2\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.10.11-py3-none-any.whl size=1588313 sha256=7e75b662c7094ea3a064b3d18692800ac7b303eb12f813e9503f6696150acd0b\n",
            "  Stored in directory: /root/.cache/pip/wheels/ec/c9/3a/068c5b80305e89c8de8b0a412e67ef2986cbad74895cfb9551\n",
            "Successfully built gym-2048 gym\n",
            "Installing collected packages: numpy, gym, gym-2048\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.17.3\n",
            "    Uninstalling gym-0.17.3:\n",
            "      Successfully uninstalled gym-0.17.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yellowbrick 1.4 requires numpy>=1.16.0, but you have numpy 1.14.6 which is incompatible.\n",
            "xarray 0.20.2 requires numpy>=1.18, but you have numpy 1.14.6 which is incompatible.\n",
            "xarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.14.6 which is incompatible.\n",
            "tifffile 2021.11.2 requires numpy>=1.15.1, but you have numpy 1.14.6 which is incompatible.\n",
            "tensorflow 2.8.2+zzzcolab20220527125636 requires numpy>=1.20, but you have numpy 1.14.6 which is incompatible.\n",
            "tables 3.7.0 requires numpy>=1.19.0, but you have numpy 1.14.6 which is incompatible.\n",
            "spacy 2.2.4 requires numpy>=1.15.0, but you have numpy 1.14.6 which is incompatible.\n",
            "seaborn 0.11.2 requires numpy>=1.15, but you have numpy 1.14.6 which is incompatible.\n",
            "scikit-image 0.18.3 requires numpy>=1.16.5, but you have numpy 1.14.6 which is incompatible.\n",
            "pywavelets 1.3.0 requires numpy>=1.17.3, but you have numpy 1.14.6 which is incompatible.\n",
            "pymc3 3.11.4 requires numpy>=1.15.0, but you have numpy 1.14.6 which is incompatible.\n",
            "pyerfa 2.0.0.1 requires numpy>=1.17, but you have numpy 1.14.6 which is incompatible.\n",
            "pyarrow 6.0.1 requires numpy>=1.16.6, but you have numpy 1.14.6 which is incompatible.\n",
            "plotnine 0.6.0 requires numpy>=1.16.0, but you have numpy 1.14.6 which is incompatible.\n",
            "pandas 1.3.5 requires numpy>=1.17.3; platform_machine != \"aarch64\" and platform_machine != \"arm64\" and python_version < \"3.10\", but you have numpy 1.14.6 which is incompatible.\n",
            "numba 0.51.2 requires numpy>=1.15, but you have numpy 1.14.6 which is incompatible.\n",
            "librosa 0.8.1 requires numpy>=1.15.0, but you have numpy 1.14.6 which is incompatible.\n",
            "kapre 0.3.7 requires numpy>=1.18.5, but you have numpy 1.14.6 which is incompatible.\n",
            "jaxlib 0.3.7+cuda11.cudnn805 requires numpy>=1.19, but you have numpy 1.14.6 which is incompatible.\n",
            "jax 0.3.8 requires numpy>=1.19, but you have numpy 1.14.6 which is incompatible.\n",
            "imgaug 0.2.9 requires numpy>=1.15.0, but you have numpy 1.14.6 which is incompatible.\n",
            "fbprophet 0.7.1 requires numpy>=1.15.4, but you have numpy 1.14.6 which is incompatible.\n",
            "fastai 1.0.61 requires numpy>=1.15, but you have numpy 1.14.6 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "cvxpy 1.0.31 requires numpy>=1.15, but you have numpy 1.14.6 which is incompatible.\n",
            "blis 0.4.1 requires numpy>=1.15.0, but you have numpy 1.14.6 which is incompatible.\n",
            "astropy 4.3.1 requires numpy>=1.17, but you have numpy 1.14.6 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed gym-0.10.11 gym-2048-0.2.6 numpy-1.14.6\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.__version__)"
      ],
      "metadata": {
        "id": "eA3wQC0eXz8s",
        "outputId": "1f55980a-17ae-48ea-f97f-0f357397c0cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.11.0+cu113\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQ3ULMj_7smZ"
      },
      "outputs": [],
      "source": [
        "import gym_2048\n",
        "import gym\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from collections import deque\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from random import sample\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zS2RFKBN8wKh"
      },
      "outputs": [],
      "source": [
        "#defining a neural network class for network initializations\n",
        "\n",
        "class torch_model(nn.Module):\n",
        "    def __init__(self,layers,dropout=0,normalization=False,hidden_activation='relu'):\n",
        "        super(torch_model,self).__init__()\n",
        "        self.layers = layers\n",
        "        self.dropout = dropout\n",
        "        self.normalization = normalization\n",
        "        self.hidden_activation = hidden_activation\n",
        "\n",
        "        #defining different layers\n",
        "        self.nn_layers = []\n",
        "        for i in range(len(self.layers)-2):\n",
        "            self.nn_layers.append(nn.Linear(self.layers[i],self.layers[i+1]))\n",
        "            if hidden_activation == 'relu':\n",
        "                self.nn_layers.append(nn.ReLU())\n",
        "            elif hidden_activation == 'sigmoid':\n",
        "                self.nn_layers.append(nn.Sigmoid())\n",
        "            elif hidden_activation == 'silu':\n",
        "                self.nn_layers.append(nn.SiLU())\n",
        "            elif hidden_activation == 'tanh':\n",
        "                self.nn_layers.append(nn.Tanh())\n",
        "            elif hidden_activation == 'celu':\n",
        "                self.nn_layers.append(nn.CELU())\n",
        "            else:\n",
        "                raise Exception('activation function not recognized; available options are relu, sigmoid, silu, tanh, celu')\n",
        "            if normalization:\n",
        "                self.nn_layers.append(nn.BatchNorm1d(self.layers[i+1]))\n",
        "            self.nn_layers.append(nn.Dropout(self.dropout))\n",
        "        self.nn_layers.append(nn.Linear(self.layers[-2],self.layers[-1]))\n",
        "        self.nn_layers = nn.ModuleList(self.nn_layers)\n",
        "    def forward(self,x):\n",
        "        for layer in self.nn_layers:\n",
        "            x = layer(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X17xktlWeOGL"
      },
      "outputs": [],
      "source": [
        "def print_model(model):\n",
        "    \"\"\" \n",
        "    A simple functon that prints out a PyTorch model's structural details\n",
        "\n",
        "    PARAMETERS\n",
        "    ----------\n",
        "    model     a torch.nn.Model\n",
        "    \"\"\"\n",
        "\n",
        "    # Print model's state_dict\n",
        "    print(\"Model's state dictionary (stored weights):\")\n",
        "    for param_tensor in model.state_dict():\n",
        "        print(\"  \", param_tensor, \"\\t\", tuple(model.state_dict()[param_tensor].size()))\n",
        "\n",
        "    # Print the number of parameters in the model    \n",
        "    parameter_count =  sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(\"In total, this network has \", parameter_count, \" trainable parameters\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q406ZuWYwftx"
      },
      "outputs": [],
      "source": [
        "class rl_agent():\n",
        "    def __init__(self,kwargs):\n",
        "        self.environment = gym.make('2048-v0')\n",
        "        self.state_size = self.environment.observation_space.shape[0]*self.environment.observation_space.shape[1]\n",
        "        self.action_size = self.environment.action_space.n\n",
        "        self.episodes = kwargs['episodes']\n",
        "        self.memory = deque(maxlen=kwargs['memory_len'])\n",
        "        self.gamma = kwargs['gamma']    # discount rate\n",
        "        self.epsilon = kwargs['epsilon_start']  # exploration rate\n",
        "        self.epsilon_min = kwargs['epsilon_min']\n",
        "        self.epsilon_decay = kwargs['epsilon_decay']\n",
        "        self.batch_size = kwargs['batch_size']\n",
        "        self.train_start = kwargs['train_start']\n",
        "        self.model = torch_model(layers=kwargs['layers'],dropout=kwargs['dropout'],\n",
        "                                 normalization=kwargs['batch_norm'],hidden_activation=kwargs['activation'])\n",
        "        # print_model(self.model)\n",
        "        #defining memory to remember states, next_states, rewards, actions\n",
        "        self.memory = deque(maxlen = 10000)\n",
        "        self.loss_fn = nn.MSELoss()\n",
        "        self.optimizer = torch.optim.RMSprop(self.model.parameters(), lr=kwargs['lr'])\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "        if len(self.memory) > self.train_start and self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "    def reward_strategy(self,state,score):\n",
        "        reward = score\n",
        "        #bottom left corner should have the maximum tile\n",
        "        reward += state[3][0]\n",
        "        reward += state[3][1]//2\n",
        "        reward += state[3][2]//4\n",
        "        reward += state[3][1]//8\n",
        "        return reward\n",
        "\n",
        "        \n",
        "    #training the neural network to approximate the q function\n",
        "    def replay(self):\n",
        "        if len(self.memory) < self.train_start: #if enough examples are not present, then do not train the neural network\n",
        "            # print('model is not training')\n",
        "            return\n",
        "        # Randomly sample minibatch from the memory\n",
        "        minibatch = sample(self.memory, self.batch_size)\n",
        "        state = torch.zeros((self.batch_size, self.state_size))\n",
        "        next_state = torch.zeros((self.batch_size, self.state_size))\n",
        "        action, reward, done = [], [], []\n",
        "        # assign data into state, next_state, action, reward and done from minibatch\n",
        "        for i in range(self.batch_size):\n",
        "            state[i] = minibatch[i][0]\n",
        "            next_state[i] = minibatch[i][3]\n",
        "            action.append(minibatch[i][1])\n",
        "            reward.append(minibatch[i][2])\n",
        "            done.append(minibatch[i][4])\n",
        "        # compute value function of current(call it target) and value function of next state(call it target_next)\n",
        "        self.model.train()\n",
        "        q_target = self.model(state)\n",
        "        q_target_next = self.model(next_state)\n",
        "        for i in range(self.batch_size):\n",
        "            # correction on the Q value for the action used,\n",
        "            # if done[i] is true, then the target should be just the final reward\n",
        "            if done[i]:\n",
        "                q_target[i][action[i]] = reward[i]\n",
        "            else:\n",
        "                q_target[i][action[i]] = reward[i] + self.gamma * torch.max(q_target_next[i])\n",
        "        # Train the Neural Network with batches where target is the value function\n",
        "        # print('model is training')\n",
        "        self.optimizer.zero_grad()\n",
        "        q_values = self.model(state)\n",
        "        loss = self.loss_fn(q_values,q_target)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "\n",
        "    def training(self):\n",
        "        runs_array = []\n",
        "        for episode in range(self.episodes):\n",
        "            run = 0\n",
        "            state = torch.from_numpy(self.environment.reset()) #resetting the game environment and capturing the first state of the game\n",
        "            state = torch.reshape(state,(1,self.state_size))\n",
        "            state = state.float()\n",
        "            # print(state)\n",
        "            # print(type(state))\n",
        "            # print(state.shape)\n",
        "            done = False\n",
        "            # acc_reward = 0\n",
        "            while not done: #untill the game is not over, play the game\n",
        "                #take the epsilon greedy action\n",
        "                # with torch.no_grad():\n",
        "                # self.environment.render()\n",
        "                # time.sleep(5)\n",
        "                run += 1\n",
        "                self.model.eval()\n",
        "                q_values = self.model(state)\n",
        "                greedy_action = torch.argmax(q_values).item()\n",
        "                random_action = np.random.choice([0,1,2,3])\n",
        "                final_action = np.random.choice([greedy_action,random_action],p=[1-self.epsilon,self.epsilon])\n",
        "                #run the selected action and get the next state, reward, done\n",
        "\n",
        "                next_state, score, done, _ = self.environment.step(final_action)\n",
        "                next_state = torch.from_numpy(next_state.copy())\n",
        "                next_state = next_state.float()\n",
        "                #may need to change and test other rewarding methods like max(tiles), difference between max tile and 2048 etc\n",
        "                # acc_reward += reward\n",
        "                if done and torch.max(next_state)!= 2048:\n",
        "                    reward = -100\n",
        "                else:\n",
        "                    reward = self.reward_strategy(next_state,score) #should we consider state --> next_state in the reward strategy?\n",
        "                #storing the state-action-nextstate-reward-done touple in memory for training purpose\n",
        "                next_state = torch.reshape(next_state, (1, self.state_size))\n",
        "                self.remember(state, final_action, reward, next_state, done)\n",
        "                #updating the state to next_state\n",
        "                state = next_state\n",
        "                if done:\n",
        "                    self.environment.render()\n",
        "                    #return_rewards.append(i)\n",
        "                    dateTimeObj = datetime.now()\n",
        "                    timestampStr = dateTimeObj.strftime(\"%H:%M:%S\")\n",
        "                    print(\"episode: {}/{}, score: {}, max_tile: {},e: {:.2}, runs: {}\".format(episode+1, self.episodes, reward, torch.max(state),self.epsilon, run))\n",
        "                    # save model option\n",
        "                    # if i >= 500:\n",
        "                    #     print(\"Saving trained model as cartpole-dqn-training.h5\")\n",
        "                    #     self.save(\"./save/cartpole-dqn-training.h5\")\n",
        "                    #     return # remark this line if you want to train the model longer\n",
        "                self.replay()\n",
        "            runs_array.append(run)\n",
        "        return runs_array\n",
        "\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DhlseyjssQ0R"
      },
      "outputs": [],
      "source": [
        "kwargs = {\n",
        "    'episodes':700,\n",
        "    'gamma':0.95,\n",
        "    'epsilon_start':1.0,\n",
        "    'epsilon_min':0.000001,\n",
        "    'epsilon_decay':0.999999,\n",
        "    'batch_size':512,\n",
        "    'train_start':1000,\n",
        "    'layers':[16,2048,1024,512,256,128,64,32,4],\n",
        "    'dropout':0.0,\n",
        "    'batch_norm':False,\n",
        "    'activation':'relu',\n",
        "    'lr':0.0001,\n",
        "    'memory_len':50000\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AJZjR2hkrzBg",
        "outputId": "6ffeb48e-b6bd-4728-dd19-cb62d917ce05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym_2048/env.py:120: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
            "  board[tile_locs] = tiles\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:30: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:31: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:32: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4 \t16 \t4 \t2\n",
            "2 \t8 \t2 \t4\n",
            "128 \t32 \t64 \t16\n",
            "4 \t2 \t8 \t4\n",
            "episode: 1/700, score: -100, max_tile: 128.0,e: 1.0, runs: 155\n",
            "2 \t4 \t2 \t8\n",
            "4 \t32 \t8 \t16\n",
            "2 \t8 \t32 \t4\n",
            "4 \t2 \t8 \t2\n",
            "episode: 2/700, score: -100, max_tile: 32.0,e: 1.0, runs: 60\n",
            "2 \t8 \t2 \t8\n",
            "8 \t4 \t16 \t64\n",
            "4 \t8 \t32 \t4\n",
            "2 \t16 \t4 \t2\n",
            "episode: 3/700, score: -100, max_tile: 64.0,e: 1.0, runs: 81\n",
            "4 \t8 \t16 \t2\n",
            "2 \t4 \t32 \t8\n",
            "8 \t32 \t2 \t16\n",
            "2 \t4 \t8 \t2\n",
            "episode: 4/700, score: -100, max_tile: 32.0,e: 1.0, runs: 66\n",
            "8 \t2 \t8 \t2\n",
            "32 \t8 \t2 \t8\n",
            "4 \t2 \t8 \t2\n",
            "2 \t4 \t2 \t8\n",
            "episode: 5/700, score: -100, max_tile: 32.0,e: 1.0, runs: 40\n",
            "2 \t16 \t4 \t2\n",
            "4 \t2 \t128 \t8\n",
            "2 \t128 \t8 \t4\n",
            "16 \t32 \t4 \t2\n",
            "episode: 6/700, score: -100, max_tile: 128.0,e: 1.0, runs: 157\n",
            "2 \t4 \t2 \t4\n",
            "8 \t128 \t32 \t8\n",
            "2 \t4 \t2 \t4\n",
            "4 \t2 \t4 \t2\n",
            "episode: 7/700, score: -100, max_tile: 128.0,e: 1.0, runs: 96\n",
            "4 \t32 \t8 \t2\n",
            "2 \t4 \t64 \t4\n",
            "32 \t8 \t2 \t16\n",
            "2 \t4 \t16 \t2\n",
            "episode: 8/700, score: -100, max_tile: 64.0,e: 1.0, runs: 92\n",
            "8 \t4 \t16 \t4\n",
            "32 \t8 \t2 \t32\n",
            "2 \t128 \t32 \t2\n",
            "4 \t16 \t8 \t4\n",
            "episode: 9/700, score: -100, max_tile: 128.0,e: 1.0, runs: 146\n",
            "4 \t16 \t64 \t8\n",
            "8 \t32 \t4 \t16\n",
            "2 \t4 \t16 \t4\n",
            "4 \t2 \t4 \t2\n",
            "episode: 10/700, score: -100, max_tile: 64.0,e: 1.0, runs: 84\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-01bdb0a2df64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# for run in range(runs):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrl_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mruns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m701\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mruns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'# game plays before the game ends'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-f38c4345c061>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    104\u001b[0m                     \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                     \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#should we consider state --> next_state in the reward strategy?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m                 \u001b[0;31m#storing the state-action-nextstate-reward-done touple in memory for training purpose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-f38c4345c061>\u001b[0m in \u001b[0;36mreward_strategy\u001b[0;34m(self, state, score)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m#bottom left corner should have the maximum tile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mreward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mreward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mreward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mreward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0massigned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWRAPPER_ASSIGNMENTS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0massigned\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0massigned\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # rewards_final = []\n",
        "    # runs = 20\n",
        "    # for run in range(runs):\n",
        "    agent = rl_agent(kwargs)\n",
        "    runs = agent.training()\n",
        "    plt.plot(range(1,701),runs)\n",
        "    plt.title('# game plays before the game ends')\n",
        "    plt.xlabel('episode number')\n",
        "    plt.plot('# game plays')\n",
        "    plt.show()\n",
        "        # rewards_final.append(agent.training())\n",
        "    # rewards_final  =np.array(rewards_final)\n",
        "    # rewards_avg = np.average(rewards_final,axis = 0)\n",
        "    # rewards_std = np.std(rewards_final,axis = 0)\n",
        "    # plt.plot(list(range(1,151)),rewards_avg,color = 'red',label='average')\n",
        "    # plt.fill_between(list(range(1,151)),rewards_avg-rewards_std,rewards_avg+rewards_std,color='red',label='+/- 1 std',alpha=0.2)\n",
        "    # plt.xlabel('episode #')\n",
        "    # plt.ylabel('reward')\n",
        "    # plt.title('mean and std over 100 runs of reward')\n",
        "    # plt.legend()\n",
        "    # plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "DRL_Final_Project_strategy_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}