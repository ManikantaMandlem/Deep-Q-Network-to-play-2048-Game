{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ManikantaMandlem/Deep-Q-Network-to-play-2048-Game/blob/main/DRL_Final_Project_strategy_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcwVbSm3ljeM"
      },
      "source": [
        "change reward policy - max tile - not working\n",
        "reward policy - max tile at corners - check\n",
        "reward policy - tiles with big numbers to the adjacent - check\n",
        "reward policy - step by step rewarding policy - figure out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym-2048"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EjMyEAPLzq96",
        "outputId": "73daee0e-36a9-46cf-9b9e-5aa8ff7f57e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym-2048 in /usr/local/lib/python3.7/dist-packages (0.2.6)\n",
            "Requirement already satisfied: gym~=0.10.0 in /usr/local/lib/python3.7/dist-packages (from gym-2048) (0.10.11)\n",
            "Requirement already satisfied: numpy~=1.14.0 in /usr/local/lib/python3.7/dist-packages (from gym-2048) (1.14.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gym~=0.10.0->gym-2048) (1.15.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym~=0.10.0->gym-2048) (1.4.1)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.7/dist-packages (from gym~=0.10.0->gym-2048) (2.23.0)\n",
            "Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym~=0.10.0->gym-2048) (1.5.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet>=1.2.0->gym~=0.10.0->gym-2048) (0.16.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->gym~=0.10.0->gym-2048) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->gym~=0.10.0->gym-2048) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->gym~=0.10.0->gym-2048) (2022.5.18.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0->gym~=0.10.0->gym-2048) (3.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQ3ULMj_7smZ"
      },
      "outputs": [],
      "source": [
        "import gym_2048\n",
        "import gym\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from collections import deque\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from random import sample\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import math\n",
        "from scipy.special import softmax\n",
        "# import sys\n",
        "# sys.stdout = open('output.txt','wt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zS2RFKBN8wKh"
      },
      "outputs": [],
      "source": [
        "#defining a neural network class for network initializations\n",
        "\n",
        "class torch_model(nn.Module):\n",
        "    def __init__(self,layers,dropout=0,normalization=False,hidden_activation='relu'):\n",
        "        super(torch_model,self).__init__()\n",
        "        self.layers = layers\n",
        "        self.dropout = dropout\n",
        "        self.normalization = normalization\n",
        "        self.hidden_activation = hidden_activation\n",
        "\n",
        "        #defining different layers\n",
        "        self.nn_layers = []\n",
        "        for i in range(len(self.layers)-2):\n",
        "            self.nn_layers.append(nn.Linear(self.layers[i],self.layers[i+1]))\n",
        "            if hidden_activation == 'relu':\n",
        "                self.nn_layers.append(nn.ReLU())\n",
        "            elif hidden_activation == 'sigmoid':\n",
        "                self.nn_layers.append(nn.Sigmoid())\n",
        "            elif hidden_activation == 'silu':\n",
        "                self.nn_layers.append(nn.SiLU())\n",
        "            elif hidden_activation == 'tanh':\n",
        "                self.nn_layers.append(nn.Tanh())\n",
        "            elif hidden_activation == 'celu':\n",
        "                self.nn_layers.append(nn.CELU())\n",
        "            else:\n",
        "                raise Exception('activation function not recognized; available options are relu, sigmoid, silu, tanh, celu')\n",
        "            if normalization:\n",
        "                self.nn_layers.append(nn.BatchNorm1d(self.layers[i+1]))\n",
        "            self.nn_layers.append(nn.Dropout(self.dropout))\n",
        "        self.nn_layers.append(nn.Linear(self.layers[-2],self.layers[-1]))\n",
        "        self.nn_layers = nn.ModuleList(self.nn_layers)\n",
        "    def forward(self,x):\n",
        "        for layer in self.nn_layers:\n",
        "            x = layer(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X17xktlWeOGL"
      },
      "outputs": [],
      "source": [
        "def print_model(model):\n",
        "    \"\"\" \n",
        "    A simple functon that prints out a PyTorch model's structural details\n",
        "\n",
        "    PARAMETERS\n",
        "    ----------\n",
        "    model     a torch.nn.Model\n",
        "    \"\"\"\n",
        "\n",
        "    # Print model's state_dict\n",
        "    print(\"Model's state dictionary (stored weights):\")\n",
        "    for param_tensor in model.state_dict():\n",
        "        print(\"  \", param_tensor, \"\\t\", tuple(model.state_dict()[param_tensor].size()))\n",
        "\n",
        "    # Print the number of parameters in the model    \n",
        "    parameter_count =  sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(\"In total, this network has \", parameter_count, \" trainable parameters\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q406ZuWYwftx"
      },
      "outputs": [],
      "source": [
        "class rl_agent():\n",
        "    def __init__(self,kwargs):\n",
        "        self.environment = gym.make('2048-v0')\n",
        "        self.state_size = self.environment.observation_space.shape[0]*self.environment.observation_space.shape[1]\n",
        "        self.action_size = self.environment.action_space.n\n",
        "        self.episodes = kwargs['episodes']\n",
        "        self.memory = deque(maxlen=kwargs['memory_len'])\n",
        "        self.gamma = kwargs['gamma']    # discount rate\n",
        "        self.epsilon = kwargs['epsilon_start']  # exploration rate\n",
        "        self.epsilon_min = kwargs['epsilon_min']\n",
        "        self.epsilon_decay = kwargs['epsilon_decay']\n",
        "        self.batch_size = kwargs['batch_size']\n",
        "        self.train_start = kwargs['train_start']\n",
        "        self.model = torch_model(layers=kwargs['layers'],dropout=kwargs['dropout'],\n",
        "                                 normalization=kwargs['batch_norm'],hidden_activation=kwargs['activation'])\n",
        "        # print_model(self.model)\n",
        "        #defining memory to remember states, next_states, rewards, actions\n",
        "        self.memory = deque(maxlen = 10000)\n",
        "        self.loss_fn = nn.MSELoss()\n",
        "        self.optimizer = torch.optim.RMSprop(self.model.parameters(), lr=kwargs['lr'])\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "        if len(self.memory) > self.train_start and self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "    def reward_strategy(self,state,score):\n",
        "        reward = score\n",
        "        #bottom left corner should have the maximum tile\n",
        "        reward += state[3][0]\n",
        "        reward += state[3][1]//2\n",
        "        reward += state[3][2]//4\n",
        "        reward += state[3][1]//8\n",
        "        return reward\n",
        "\n",
        "        \n",
        "    #training the neural network to approximate the q function\n",
        "    def replay(self):\n",
        "        if len(self.memory) < self.train_start: #if enough examples are not present, then do not train the neural network\n",
        "            # print('model is not training')\n",
        "            return\n",
        "        # Randomly sample minibatch from the memory\n",
        "        minibatch = sample(self.memory, self.batch_size)\n",
        "        state = torch.zeros((self.batch_size, self.state_size))\n",
        "        next_state = torch.zeros((self.batch_size, self.state_size))\n",
        "        action, reward, done = [], [], []\n",
        "        # assign data into state, next_state, action, reward and done from minibatch\n",
        "        for i in range(self.batch_size):\n",
        "            state[i] = minibatch[i][0]\n",
        "            next_state[i] = minibatch[i][3]\n",
        "            action.append(minibatch[i][1])\n",
        "            reward.append(minibatch[i][2])\n",
        "            done.append(minibatch[i][4])\n",
        "        # compute value function of current(call it target) and value function of next state(call it target_next)\n",
        "        self.model.train()\n",
        "        q_target = self.model(state)\n",
        "        q_target_next = self.model(next_state)\n",
        "        for i in range(self.batch_size):\n",
        "            # correction on the Q value for the action used,\n",
        "            # if done[i] is true, then the target should be just the final reward\n",
        "            if done[i]:\n",
        "                q_target[i][action[i]] = reward[i]\n",
        "            else:\n",
        "                q_target[i][action[i]] = reward[i] + self.gamma * torch.max(q_target_next[i])\n",
        "        # Train the Neural Network with batches where target is the value function\n",
        "        # print('model is training')\n",
        "        self.optimizer.zero_grad()\n",
        "        q_values = self.model(state)\n",
        "        loss = self.loss_fn(q_values,q_target)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "\n",
        "    def training(self):\n",
        "        # print('test')\n",
        "        runs_array = []\n",
        "        for episode in range(self.episodes):\n",
        "            run = 0\n",
        "            state = torch.from_numpy(self.environment.reset()) #resetting the game environment and capturing the first state of the game\n",
        "            state = torch.reshape(state,(1,self.state_size))\n",
        "            state = state.float()\n",
        "            # print(state)\n",
        "            # print(type(state))\n",
        "            # print(state.shape)\n",
        "            done = False\n",
        "            # acc_reward = 0\n",
        "            while not done: #untill the game is not over, play the game\n",
        "                #take the epsilon greedy action\n",
        "                # with torch.no_grad():\n",
        "                # self.environment.render()\n",
        "                # time.sleep(5)\n",
        "                run += 1\n",
        "                self.model.eval()\n",
        "                q_values = self.model(state)\n",
        "                # print(q_values)\n",
        "                # greedy_action = torch.argmax(q_values).item()\n",
        "                # random_action = np.random.choice([0,1,2,3])\n",
        "                # final_action = np.random.choice([greedy_action,random_action],p=[1-self.epsilon,self.epsilon])\n",
        "                #run the selected action and get the next state, reward, done\n",
        "                probs = q_values.detach().numpy()\n",
        "                probs = probs[0]\n",
        "                probs = softmax(probs)\n",
        "                # print(probs)\n",
        "                # print(self.epsilon)\n",
        "                # self.environment.render()\n",
        "                final_action = np.random.choice([0,1,2,3],p=probs)\n",
        "                next_state, score, done, _ = self.environment.step(final_action)\n",
        "                next_state = torch.from_numpy(next_state.copy())\n",
        "                next_state = next_state.float()\n",
        "                #may need to change and test other rewarding methods like max(tiles), difference between max tile and 2048 etc\n",
        "                # acc_reward += reward\n",
        "                reward = self.reward_strategy(next_state,score) #should we consider state --> next_state in the reward strategy?\n",
        "                #storing the state-action-nextstate-reward-done touple in memory for training purpose\n",
        "                next_state = torch.reshape(next_state, (1, self.state_size))\n",
        "                self.remember(state, final_action, reward, next_state, done)\n",
        "                #updating the state to next_state\n",
        "                state = next_state\n",
        "                if done:\n",
        "                    self.environment.render()\n",
        "                    #return_rewards.append(i)\n",
        "                    dateTimeObj = datetime.now()\n",
        "                    timestampStr = dateTimeObj.strftime(\"%H:%M:%S\")\n",
        "                    print(\"episode: {}/{}, score: {}, max_tile: {},e: {:.2}, runs: {}\".format(episode+1, self.episodes, reward, torch.max(state),self.epsilon, run))\n",
        "                    # save model option\n",
        "                    # if i >= 500:\n",
        "                    #     print(\"Saving trained model as cartpole-dqn-training.h5\")\n",
        "                    #     self.save(\"./save/cartpole-dqn-training.h5\")\n",
        "                    #     return # remark this line if you want to train the model longer\n",
        "                self.replay()\n",
        "            runs_array.append(run)\n",
        "        return runs_array\n",
        "\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DhlseyjssQ0R"
      },
      "outputs": [],
      "source": [
        "kwargs = {\n",
        "    'episodes':700,\n",
        "    'gamma':0.9,\n",
        "    'epsilon_start':1.0,\n",
        "    'epsilon_min':0.000001,\n",
        "    'epsilon_decay':0.999999,\n",
        "    'batch_size':512,\n",
        "    'train_start':1000,\n",
        "    'layers':[16,2048,1024,512,256,128,64,32,4],\n",
        "    'dropout':0.0,\n",
        "    'batch_norm':False,\n",
        "    'activation':'relu',\n",
        "    'lr':0.0001,\n",
        "    'memory_len':50000\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AJZjR2hkrzBg",
        "outputId": "c63db38a-65f8-480d-c181-8272e8c5fa55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:30: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:31: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:32: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2 \t4 \t8 \t4\n",
            "4 \t8 \t64 \t2\n",
            "8 \t64 \t16 \t4\n",
            "4 \t8 \t128 \t8\n",
            "episode: 1/700, score: 45.0, max_tile: 128.0,e: 1.0, runs: 173\n",
            "2 \t4 \t2 \t4\n",
            "4 \t8 \t16 \t2\n",
            "2 \t128 \t64 \t16\n",
            "4 \t32 \t4 \t2\n",
            "episode: 2/700, score: 25.0, max_tile: 128.0,e: 1.0, runs: 138\n",
            "2 \t4 \t8 \t4\n",
            "4 \t32 \t16 \t8\n",
            "16 \t64 \t32 \t2\n",
            "2 \t16 \t2 \t4\n",
            "episode: 3/700, score: 16.0, max_tile: 64.0,e: 1.0, runs: 109\n",
            "2 \t8 \t4 \t2\n",
            "16 \t4 \t16 \t64\n",
            "2 \t16 \t32 \t4\n",
            "4 \t2 \t4 \t8\n",
            "episode: 4/700, score: 6.0, max_tile: 64.0,e: 1.0, runs: 81\n",
            "2 \t4 \t16 \t8\n",
            "4 \t64 \t8 \t2\n",
            "16 \t2 \t16 \t32\n",
            "4 \t32 \t2 \t4\n",
            "episode: 5/700, score: 28.0, max_tile: 64.0,e: 1.0, runs: 91\n",
            "2 \t32 \t2 \t32\n",
            "4 \t8 \t4 \t2\n",
            "8 \t4 \t32 \t4\n",
            "4 \t2 \t8 \t32\n",
            "episode: 6/700, score: 15.0, max_tile: 32.0,e: 1.0, runs: 76\n",
            "4 \t8 \t4 \t2\n",
            "2 \t64 \t16 \t8\n",
            "16 \t128 \t4 \t16\n",
            "8 \t2 \t8 \t4\n",
            "episode: 7/700, score: 15.0, max_tile: 128.0,e: 1.0, runs: 148\n",
            "2 \t32 \t2 \t4\n",
            "4 \t16 \t8 \t64\n",
            "16 \t64 \t32 \t8\n",
            "4 \t16 \t4 \t2\n",
            "episode: 8/700, score: 19.0, max_tile: 64.0,e: 1.0, runs: 146\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-b22e4fe2ec35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# for run in range(runs):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrl_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mruns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;31m# plt.plot(range(1,201),runs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# plt.title('# game plays before the game ends')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-40b3bd128458>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    126\u001b[0m                     \u001b[0;31m#     self.save(\"./save/cartpole-dqn-training.h5\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                     \u001b[0;31m#     return # remark this line if you want to train the model longer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m             \u001b[0mruns_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mruns_array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-40b3bd128458>\u001b[0m in \u001b[0;36mreplay\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mq_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mq_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # rewards_final = []\n",
        "    # runs = 20\n",
        "    # for run in range(runs):\n",
        "    agent = rl_agent(kwargs)\n",
        "    runs = agent.training()\n",
        "    plt.plot(range(1,701),runs)\n",
        "    plt.title('# game plays before the game ends')\n",
        "    plt.xlabel('episode number')\n",
        "    plt.plot('# game plays')\n",
        "    plt.show()\n",
        "        # rewards_final.append(agent.training())\n",
        "    # rewards_final  =np.array(rewards_final)\n",
        "    # rewards_avg = np.average(rewards_final,axis = 0)\n",
        "    # rewards_std = np.std(rewards_final,axis = 0)\n",
        "    # plt.plot(list(range(1,151)),rewards_avg,color = 'red',label='average')\n",
        "    # plt.fill_between(list(range(1,151)),rewards_avg-rewards_std,rewards_avg+rewards_std,color='red',label='+/- 1 std',alpha=0.2)\n",
        "    # plt.xlabel('episode #')\n",
        "    # plt.ylabel('reward')\n",
        "    # plt.title('mean and std over 100 runs of reward')\n",
        "    # plt.legend()\n",
        "    # plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "DRL_Final_Project_strategy_3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}